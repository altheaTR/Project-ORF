{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0vl+rP59RjpzwDRNpAgMd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "sqViA75IBwnb",
        "outputId": "1e82e044-14d9-4e8c-b32a-67a1257ed276"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-7-16cedae63e7c>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-16cedae63e7c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    FROM python:3.12-slim\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "FROM python:3.12-slim\n",
        "\n",
        "RUN apt-get update && \\\n",
        "    apt-get install -y wget libgomp1\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "RUN wget https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.16.0+-x64-linux.tar.gz && \\\n",
        "    tar -xzvf ncbi-blast-2.16.0+-x64-linux.tar.gz && \\\n",
        "    mv ncbi-blast-2.16.0+ /usr/local/ && \\\n",
        "    rm ncbi-blast-2.16.0+-x64-linux.tar.gz\n",
        "\n",
        "ENV PATH=\"/usr/local/ncbi-blast-2.16.0+/bin:$PATH\"\n",
        "\n",
        "RUN wget https://ftp.ncbi.nlm.nih.gov/blast/db/v5/swissprot.tar.gz && \\\n",
        "    mkdir -p /db/swissprot && mv swissprot* /db/swissprot && \\\n",
        "    tar -xzvf /db/swissprot/swissprot.tar.gz -C /db/swissprot\n",
        "\n",
        "COPY . /app"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install biopython\n",
        "from Bio import SeqIO\n",
        "from Bio.Blast import NCBIXML\n",
        "\n",
        "def read_fasta(filepath):\n",
        "    try:\n",
        "        return [(record.id, str(record.seq)) for record in SeqIO.parse(filepath, \"fasta\")]\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{filepath}' not found.\")\n",
        "        return []\n",
        "\n",
        "def find_orfs_in_sequence(sequence_data, sequence_identifier):\n",
        "    start_codon = \"ATG\"\n",
        "    stop_codons = {\"TAA\", \"TAG\", \"TGA\"}\n",
        "    found_orfs = []\n",
        "    current_index = 0\n",
        "    while current_index <= len(sequence_data) - 3:\n",
        "        if sequence_data[current_index:current_index+3] == start_codon:\n",
        "            orf_start_position = current_index\n",
        "            current_index += 3\n",
        "            while current_index <= len(sequence_data) - 3:\n",
        "                codon = sequence_data[current_index:current_index+3]\n",
        "                if codon in stop_codons:\n",
        "                    orf_end_position = current_index + 3\n",
        "                    reading_frame = (orf_start_position % 3) + 1\n",
        "                    found_orfs.append({\n",
        "                        \"id\": f\"{sequence_identifier}_ORF{len(found_orfs)+1}\",\n",
        "                        \"sequence\": sequence_data[orf_start_position:orf_end_position],\n",
        "                        \"length\": orf_end_position - orf_start_position,\n",
        "                        \"source\": sequence_identifier,\n",
        "                        \"start\": orf_start_position,\n",
        "                        \"end\": orf_end_position,\n",
        "                        \"frame\": reading_frame\n",
        "                    })\n",
        "                    break\n",
        "                current_index += 3\n",
        "        else:\n",
        "            current_index += 1\n",
        "    return found_orfs\n",
        "\n",
        "def write_gff_file(orf_list, gff_filepath):\n",
        "    with open(gff_filepath, \"w\") as f:\n",
        "        f.write(\"##gff-version 3\\n\")\n",
        "        f.write(f\"##date: {os.stat(gff_filepath).st_mtime if os.path.exists(gff_filepath) else 'unknown'}\\n\")\n",
        "        for orf in orf_list:\n",
        "            f.write(\n",
        "                f\"{orf['source']}\\tORF_Pipeline\\tCDS\\t{orf['start']+1}\\t{orf['end']}\\t.\\t+\\t.\\t\"\n",
        "                f\"ID={orf['id']};length={orf['length']};frame={orf['frame']}\\n\"\n",
        "            )\n",
        "\n",
        "def write_fasta_file(orf_list, fasta_filepath):\n",
        "    with open(fasta_filepath, \"w\") as f:\n",
        "        for orf in orf_list:\n",
        "            f.write(f\">{orf['id']}\\n{orf['sequence']}\\n\")\n",
        "\n",
        "def run_orf_analysis(input_fasta_file, output_gff_file, output_fasta_file):\n",
        "    sequences_data = read_fasta(input_fasta_file)\n",
        "    all_extracted_orfs = []\n",
        "\n",
        "    for index, (seq_id, sequence_string) in enumerate(sequences_data):\n",
        "        orfs_for_this_seq = find_orfs_in_sequence(sequence_string, f\"Sequence_{index + 1}\")\n",
        "        all_extracted_orfs.extend(orfs_for_this_seq)\n",
        "\n",
        "    write_gff_file(all_extracted_orfs, output_gff_file)\n",
        "    write_fasta_file(all_extracted_orfs, output_fasta_file)\n",
        "    print(f\"Extracted {len(all_extracted_orfs)} ORFs from {len(sequences_data)} sequences.\")\n",
        "\n",
        "    return all_extracted_orfs\n",
        "\n",
        "def plot_orf_lengths(orfs_data, plot_filepath=\"orf_lengths_histogram.png\"):\n",
        "    lengths = [orf['length'] for orf in orfs_data]\n",
        "    if lengths:\n",
        "        plt.hist(lengths, bins=50, edgecolor='black')\n",
        "        plt.title(\"Distribution of ORF Lengths\")\n",
        "        plt.xlabel(\"ORF Length (bp)\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.savefig(plot_filepath)\n",
        "        plt.close()\n",
        "        print(f\"ORF length histogram saved to {plot_filepath}\")\n",
        "    else:\n",
        "        print(\"No ORFs found to plot.\")\n",
        "\n",
        "# BLAST+ Validation\n",
        "def run_blast_comparison(query_fasta_path, db_path, output_blast_path):\n",
        "    print(f\"Running BLASTp with query: {query_fasta_path}, database: {db_path}\")\n",
        "    command = [\n",
        "        \"blastp\",\n",
        "        \"-query\", query_fasta_path,\n",
        "        \"-db\", db_path,\n",
        "        \"-outfmt\", \"5\",\n",
        "        \"-taxids\", \"9606\",\n",
        "        \"-evalue\", \"1e-5\",\n",
        "        \"-out\", output_blast_path\n",
        "    ]\n",
        "    try:\n",
        "        subprocess.run(command, check=True, capture_output=True, text=True)\n",
        "        print(f\"BLASTp completed successfully. Results saved to {output_blast_path}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"BLASTp failed with error: {e}\")\n",
        "        print(f\"STDOUT: {e.stdout}\")\n",
        "        print(f\"STDERR: {e.stderr}\")\n",
        "        raise\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: 'blastp' command not found. Ensure BLAST+ is installed and in your system's PATH.\")\n",
        "        print(\"If using Docker, ensure your Dockerfile configures the PATH correctly and you are running within the container.\")\n",
        "        raise\n",
        "\n",
        "def parse_blast_xml_results(blast_xml_path):\n",
        "    print(f\"Parsing BLAST XML results from {blast_xml_path}\")\n",
        "    blast_results = {}\n",
        "    try:\n",
        "        with open(blast_xml_path, \"r\") as result_handle:\n",
        "            blast_records = NCBIXML.parse(result_handle)\n",
        "            for blast_record in blast_records:\n",
        "                query_id = blast_record.query.split(\" \", 1)[0]\n",
        "                best_hit = None\n",
        "                best_evalue = float('inf')\n",
        "\n",
        "                for alignment in blast_record.alignments:\n",
        "                    for hsp in alignment.hsps:\n",
        "                        if hsp.expect < best_evalue:\n",
        "                            best_evalue = hsp.expect\n",
        "                            best_hit = {\n",
        "                                \"subject_id\": alignment.accession,\n",
        "                                \"evalue\": hsp.expect,\n",
        "                                \"query_start_in_orf\": hsp.query_start,\n",
        "                                \"query_end_in_orf\": hsp.query_end,\n",
        "                            }\n",
        "                if best_hit:\n",
        "                    blast_results[query_id] = best_hit\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: BLAST XML file '{blast_xml_path}' not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing BLAST XML: {e}\")\n",
        "    return blast_results\n",
        "\n",
        "def update_gff_with_cds(original_gff_path, updated_gff_path, orf_data, blast_validation_results):\n",
        "    print(f\"Updating GFF file {original_gff_path} with CDS information...\")\n",
        "    updated_lines = []\n",
        "    with open(original_gff_path, 'r') as f_in:\n",
        "        for line in f_in:\n",
        "            updated_lines.append(line.strip())\n",
        "\n",
        "    orf_info_map = {orf['id']: orf for orf in orf_data}\n",
        "\n",
        "    for orf_id, blast_hit in blast_validation_results.items():\n",
        "        if orf_id in orf_info_map:\n",
        "            original_orf = orf_info_map[orf_id]\n",
        "\n",
        "            cds_start_on_contig = original_orf['start'] + blast_hit['query_start_in_orf']\n",
        "            cds_end_on_contig = original_orf['start'] + blast_hit['query_end_in_orf']\n",
        "\n",
        "            if cds_start_on_contig > cds_end_on_contig:\n",
        "                 cds_start_on_contig, cds_end_on_contig = cds_end_on_contig, cds_start_on_contig\n",
        "\n",
        "            updated_lines.append(\n",
        "                f\"{original_orf['source']}\\tBLAST_Validation\\tCDS\\t{cds_start_on_contig}\\t{cds_end_on_contig}\\t\"\n",
        "                f\".\\t+\\t.\\tParent={orf_id};BlastHitID={blast_hit['subject_id']}\"\n",
        "            )\n",
        "\n",
        "            if blast_hit['query_start_in_orf'] > 1:\n",
        "                utr5_start = original_orf['start'] + 1\n",
        "                utr5_end = original_orf['start'] + blast_hit['query_start_in_orf'] - 1\n",
        "                updated_lines.append(\n",
        "                    f\"{original_orf['source']}\\tORF_Pipeline\\t5_prime_UTR\\t{utr5_start}\\t{utr5_end}\\t\"\n",
        "                    f\".\\t+\\t.\\tParent={orf_id}\"\n",
        "                )\n",
        "            if blast_hit['query_end_in_orf'] < original_orf['length']:\n",
        "                utr3_start = original_orf['start'] + blast_hit['query_end_in_orf'] + 1\n",
        "                utr3_end = original_orf['end']\n",
        "                updated_lines.append(\n",
        "                    f\"{original_orf['source']}\\tORF_Pipeline\\t3_prime_UTR\\t{utr3_start}\\t{utr3_end}\\t\"\n",
        "                    f\".\\t+\\t.\\tParent={orf_id}\"\n",
        "                )\n",
        "\n",
        "    with open(updated_gff_path, 'w') as f_out:\n",
        "        for line in updated_lines:\n",
        "            f_out.write(line + '\\n')\n",
        "    print(f\"Updated GFF with CDS features saved to {updated_gff_path}\")\n",
        "\n",
        "def classify_orfs_by_blast(orf_data, blast_validation_results):\n",
        "    validated_orfs = []\n",
        "    unvalidated_orfs = []\n",
        "    for orf in orf_data:\n",
        "        if orf['id'] in blast_validation_results:\n",
        "            validated_orfs.append(orf)\n",
        "        else:\n",
        "            unvalidated_orfs.append(orf)\n",
        "    return validated_orfs, unvalidated_orfs\n",
        "\n",
        "def plot_true_false_positive_lengths(validated_orfs, unvalidated_orfs, plot_filepath=\"orf_true_false_positive_lengths.png\"):\n",
        "    validated_lengths = [orf['length'] for orf in validated_orfs]\n",
        "    unvalidated_lengths = [orf['length'] for orf in unvalidated_orfs]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if validated_lengths:\n",
        "        plt.hist(validated_lengths, bins=50, alpha=0.7, label='Validated ORFs (True Positives)', color='skyblue', edgecolor='black')\n",
        "    if unvalidated_lengths:\n",
        "        plt.hist(unvalidated_lengths, bins=50, alpha=0.7, label='Unvalidated ORFs (False Positives)', color='salmon', edgecolor='black')\n",
        "\n",
        "    if not validated_lengths and not unvalidated_lengths:\n",
        "        print(\"No ORFs to plot for true/false positives.\")\n",
        "        plt.close()\n",
        "        return\n",
        "\n",
        "    plt.title(\"Distribution of ORF Lengths: Validated vs. Unvalidated\")\n",
        "    plt.xlabel(\"ORF Length (bp)\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', alpha=0.75)\n",
        "    plt.savefig(plot_filepath)\n",
        "    plt.close()\n",
        "    print(f\"True/False positive length histogram saved to {plot_filepath}\")\n",
        "\n",
        "def calculate_expected_orf_length_geometric():\n",
        "    probability_of_stop_codon = 3 / 64\n",
        "    expected_length_codons = 1 / probability_of_stop_codon\n",
        "    expected_length_bp = expected_length_codons * 3\n",
        "    print(f\"Expected ORF length (codons, geometric model): {expected_length_codons:.2f}\")\n",
        "    print(f\"Expected ORF length (base pairs, geometric model): {expected_length_bp:.2f}\")\n",
        "    return expected_length_codons, expected_length_bp\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    input_fasta = \"Homo_sapiens_cdna_assembed.fasta\"\n",
        "    output_gff = \"extracted_orfs.gff\"\n",
        "    output_fasta = \"extracted_orfs.fasta\"\n",
        "    updated_gff_with_cds = \"extracted_orfs_with_cds.gff\"\n",
        "\n",
        "    blast_db_path = \"/db/swissprot/swissprot\"\n",
        "    blast_output_xml = \"blast_results.xml\"\n",
        "\n",
        "    # Step 1: Extract ORFs and generate initial files\n",
        "    final_orfs = run_orf_analysis(input_fasta, output_gff, output_fasta)\n",
        "\n",
        "    # Step 2: Plot overall ORF lengths\n",
        "    plot_orf_lengths(final_orfs)\n",
        "\n",
        "    # Step 3: Validate ORFs with BLAST+\n",
        "    try:\n",
        "        run_blast_comparison(output_fasta, blast_db_path, blast_output_xml)\n",
        "        blast_hits = parse_blast_xml_results(blast_output_xml)\n",
        "\n",
        "        update_gff_with_cds(output_gff, updated_gff_with_cds, final_orfs, blast_hits)\n",
        "\n",
        "        validated_orfs, unvalidated_orfs = classify_orfs_by_blast(final_orfs, blast_hits)\n",
        "        plot_true_false_positive_lengths(validated_orfs, unvalidated_orfs)\n",
        "\n",
        "        print(f\"\\nSummary of BLAST validation:\")\n",
        "        print(f\"Total predicted ORFs: {len(final_orfs)}\")\n",
        "        print(f\"Validated ORFs (True Positives): {len(validated_orfs)}\")\n",
        "        print(f\"Unvalidated ORFs (False Positives): {len(unvalidated_orfs)}\")\n",
        "        if len(final_orfs) > 0:\n",
        "            false_positive_rate = len(unvalidated_orfs) / len(final_orfs) * 100\n",
        "            print(f\"Estimated False Positive Rate: {false_positive_rate:.2f}%\")\n",
        "        else:\n",
        "            print(\"No ORFs predicted, so no false positive rate to calculate.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nSkipping BLAST-related steps due to an error: {e}\")\n",
        "        print(\"Please ensure BLAST+ is installed and configured correctly in your Docker environment.\")\n",
        "\n",
        "    # Step 4: Estimate mean ORF length using geometric distribution\n",
        "    calculate_expected_orf_length_geometric()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_A5Ey6GBzeS",
        "outputId": "b8f54dfd-43e1-46cd-f6f5-7cdaeff1e803"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (2.0.2)\n",
            "Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m145.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.85\n",
            "Extracted 50959 ORFs from 5497 sequences.\n",
            "ORF length histogram saved to orf_lengths_histogram.png\n",
            "Running BLASTp with query: extracted_orfs.fasta, database: /db/swissprot/swissprot\n",
            "Error: 'blastp' command not found. Ensure BLAST+ is installed and in your system's PATH.\n",
            "If using Docker, ensure your Dockerfile configures the PATH correctly and you are running within the container.\n",
            "\n",
            "Skipping BLAST-related steps due to an error: [Errno 2] No such file or directory: 'blastp'\n",
            "Please ensure BLAST+ is installed and configured correctly in your Docker environment.\n",
            "Expected ORF length (codons, geometric model): 21.33\n",
            "Expected ORF length (base pairs, geometric model): 64.00\n"
          ]
        }
      ]
    }
  ]
}